{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/mambaforge-pypy3/envs/molmo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.62s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    # 'allenai/Molmo-7B-D-0924',\n",
    "    'molmo_7b_d_0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # 'allenai/Molmo-7B-D-0924',\n",
    "    'molmo_7b_d_0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    \"sample_images/0a6ee446579d2885.jpg\",\n",
    "    \"sample_images/0a8caaad03cfd733.jpg\",\n",
    "    # Add more image URLs as needed\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    \"Describe this image.\",\n",
    "    \"What do you see in this picture?\",\n",
    "    # Add more prompts corresponding to each image\n",
    "]\n",
    "\n",
    "# Load images\n",
    "images = [Image.open(url) for url in image_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Move inputs to the correct device and make a batch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m batch_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch_inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/molmo_7b_d_0924/preprocessing_molmo.py:130\u001b[0m, in \u001b[0;36mMolmoProcessor.process\u001b[0;34m(self, text, images, tokens, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m output_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_kwargs(\n\u001b[1;32m    124\u001b[0m     MolmoProcessorKwargs,\n\u001b[1;32m    125\u001b[0m     tokenizer_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39minit_kwargs,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tokens_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malways_start_with_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m image_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_token_ids[IMAGE_PROMPT]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/molmo_7b_d_0924/preprocessing_molmo.py:104\u001b[0m, in \u001b[0;36mMolmoProcessor.get_tokens_input\u001b[0;34m(self, prompt, message_format, always_start_with_space)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Assistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "batch_inputs = processor.process(images=images, text=prompts)\n",
    "\n",
    "# Move inputs to the correct device and make a batch\n",
    "batch_inputs = {k: v.to(model.device) for k, v in batch_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image 1 description:\n",
      " This image captures a young black Labrador puppy, likely around six months old, sitting on a weathered wooden deck. The puppy's sleek, short fur is entirely black, including its nose, eyes, and ears, which are slightly floppy. The dog is positioned in the center of the frame, looking up directly at the camera with a curious and attentive expression. Its front paws are visible, with one slightly tucked under its body, while its back paws are hidden from view. The wooden deck beneath the puppy is made of light brown planks with visible knots and signs of wear, adding a rustic charm to the scene. The overall composition is simple yet striking, with the puppy's glossy black coat contrasting beautifully against the light wooden background.\n",
      "\n",
      "Image 2 description:\n",
      " This black and white photograph captures a dense urban landscape, likely New York City, characterized by a multitude of skyscrapers and high-rise buildings. The image, taken from a high vantage point, possibly from a helicopter or a tall building, offers a sweeping view of the city's skyline. The buildings vary in height and architectural style, with some featuring pointed tops and others flat roofs. The sky above is filled with clouds, adding a dramatic backdrop to the scene. The photograph is devoid of any text, people, or animals, focusing solely on the architectural elements and the expansive sky. The monochromatic palette emphasizes the contrast between the buildings and the sky, creating a striking visual composition. \n",
      "(www.Flydreamers.com)\n",
      "\n",
      "Image 3 description:\n",
      " In this detailed photograph, a person's hand, likely a woman's due to the red nail polish on the thumb, is gently holding a dandelion that has gone to seed. The dandelion, with its delicate white, fluffy seeds ready to be blown away, is positioned centrally in the image. The background is dark and blurred, suggesting an outdoor setting with hints of green grass and possibly some brown leaves, indicating a natural environment. The lighting is focused on the hand and the dandelion, highlighting the intricate details of the seeds and the texture of the hand. The overall composition draws attention to the moment of transformation, capturing the essence of the dandelion's seeds preparing to take flight.This photograph is part of a series\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# Load processor and model (same as before)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    # 'allenai/Molmo-7B-D-0924',\n",
    "    'molmo_7b_d_0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # 'allenai/Molmo-7B-D-0924',\n",
    "    'molmo_7b_d_0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Create a batch of images and prompts\n",
    "image_urls = [\n",
    "    \"https://picsum.photos/id/237/536/354\",\n",
    "    \"https://picsum.photos/id/238/536/354\",\n",
    "    \"https://picsum.photos/id/239/536/354\"\n",
    "]\n",
    "\n",
    "images = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]\n",
    "prompts = [\"Describe this image.\"] * len(images)\n",
    "\n",
    "# Process each image-prompt pair and collect inputs\n",
    "batch_inputs = []\n",
    "for img, prompt in zip(images, prompts):\n",
    "    inputs = processor.process(\n",
    "        images=[img],\n",
    "        text=prompt\n",
    "    )\n",
    "    batch_inputs.append({k: v.to(model.device) for k, v in inputs.items()})\n",
    "\n",
    "# Create batched tensors\n",
    "batched_inputs = {\n",
    "    k: torch.stack([inputs[k] for inputs in batch_inputs])\n",
    "    for k in batch_inputs[0].keys()\n",
    "}\n",
    "\n",
    "# Generate outputs for the entire batch\n",
    "outputs = model.generate_from_batch(\n",
    "    batched_inputs,\n",
    "    GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n",
    "\n",
    "# Process generated outputs\n",
    "generated_texts = []\n",
    "for i, output in enumerate(outputs):\n",
    "    generated_tokens = output[batched_inputs['input_ids'].size(1):]\n",
    "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    generated_texts.append(generated_text)\n",
    "\n",
    "# Print results\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"\\nImage {i + 1} description:\")\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image 1 description:\n",
      " This image captures a young black Labrador puppy, likely around six months old, sitting on a weathered\n",
      "\n",
      "Image 2 description:\n",
      " This black and white photograph captures a dense urban landscape, likely New York City, characterized by a multitude\n",
      "\n",
      "Image 3 description:\n",
      " In this detailed photograph, a person's hand, likely a woman's due to the red nail polish\n"
     ]
    }
   ],
   "source": [
    "# Generate outputs for the entire batch\n",
    "outputs = model.generate_from_batch(\n",
    "    batched_inputs,\n",
    "    GenerationConfig(max_new_tokens=20, stop_strings=\"<|endoftext|>\"),\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n",
    "\n",
    "# Process generated outputs\n",
    "generated_texts = []\n",
    "for i, output in enumerate(outputs):\n",
    "    generated_tokens = output[batched_inputs['input_ids'].size(1):]\n",
    "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    generated_texts.append(generated_text)\n",
    "\n",
    "# Print results\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"\\nImage {i + 1} description:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
