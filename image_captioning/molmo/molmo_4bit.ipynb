{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/mambaforge-pypy3/envs/cv-docker-images/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-30 14:44:54.403794: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 14:44:54.412425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730270694.423642   73721 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730270694.426992   73721 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 14:44:54.438616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "repo_name = \"cyan2k/molmo-7B-D-bnb-4bit\"\n",
    "arguments = {\"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"trust_remote_code\": True}\n",
    "\n",
    "\n",
    "# load the processor\n",
    "processor = AutoProcessor.from_pretrained(repo_name, **arguments)\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_name, **arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1730270702.213889   73721 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image depicts a woman, likely in her 40s or 50s, standing\n",
      "CPU times: user 6.03 s, sys: 949 ms, total: 6.98 s\n",
      "Wall time: 6.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load image and prompt\n",
    "inputs = processor.process(\n",
    "    images=[Image.open(\"sample_images/0a6ee446579d2885.jpg\"), Image.open(\"sample_images/0a6ee446579d2885.jpg\"), Image.open(\"sample_images/0a6ee446579d2885.jpg\")],\n",
    "    text=\"Describe this image.\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "# generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\n",
    "output = model.generate_from_batch(\n",
    "    inputs,\n",
    "    GenerationConfig(max_new_tokens=20, stop_strings=\"<|endoftext|>\"),\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# only get generated tokens; decode them to text\n",
    "generated_tokens = output[0, inputs[\"input_ids\"].size(1) :]\n",
    "generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The image depicts a woman, likely in her 40s or 50s, standing'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-docker-images",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
